---
title: "STA141A Final Project"
author: "Animay Sharma, Aditya Kallepalli, Charles Chien"
date: "12/14/2020"
output:
  html_document:
    df_print: paged
---

```{r, message=FALSE, warning = FALSE, echo=FALSE}
#setup
set.seed(123)
library(readr)
library(tidyverse)
library(fastDummies)
library(knitr)
library(plyr)
library(dplyr)
library(explore)
library(corrplot)
library(ROCR)
library(cutpointr)
library(caret)
library(randomForest)
require(caTools)
library(rpart)


#import dataset
bank.data <- read_delim("datasets/bank-full.csv",";", escape_double = FALSE, trim_ws = TRUE)
#clean NA vaules
bank.data = na.omit(bank.data)
#look at sample of our dataset

head(bank.data)

# modifying data set
#cat_data = data.frame(bank.data$job,bank.data$marital,bank.data$education)
#bin_cat_data = dummy_cols(cat_data)
#bin_cat_data = bin_cat_data %>% select(4:22)

#yesno_data = data.frame(bank.data$default,bank.data$housing,bank.data$loan,bank.data$y)
#yesno_data$bank.data.default <- revalue(yesno_data$bank.data.default, c("yes"=1))
#yesno_data$bank.data.default <- revalue(yesno_data$bank.data.default, c("no"=0))
#yesno_data$bank.data.housing <- revalue(yesno_data$bank.data.housing, c("yes"=1))
#yesno_data$bank.data.housing <- revalue(yesno_data$bank.data.housing, c("no"=0))
#yesno_data$bank.data.loan <- revalue(yesno_data$bank.data.loan, c("yes"=1))
#yesno_data$bank.data.loan <- revalue(yesno_data$bank.data.loan, c("no"=0))
#yesno_data$bank.data.y <- revalue(yesno_data$bank.data.y, c("yes"=1))
#yesno_data$bank.data.y <- revalue(yesno_data$bank.data.y, c("no"=0))
#remaining_data = bank.data %>% select(1,6,9,10,11,12,13,14,15,16)
#master_bin_data = cbind(bin_cat_data,yesno_data,remaining_data)
#head(master_bin_data)

#create test and train dataset
bank.train = bank.data %>%
  sample_frac(0.5)

bank.test = bank.data %>%
  setdiff(bank.train)

bank.test$y <- ifelse(bank.test$y=='yes',1,0)
```

# 1. Introduction
## 1.1 Background
Here, we have a set of marketing data of a banking institution. The data is related with direct marketing campaigns of a Portuguese banking institution. The marketing campaigns were based on phone calls. Often, more than one contact to the same client was required, in order to access if the product (bank term deposit) would be ('yes') or not ('no') subscribed.

Among the four datasets provided, we chose to utilize the "bank-full.csv" in this report, with 17 different inputs.

As mentioned in article (Lopez, Customer segmentation using machine learning 2020) [0], data science and machine learning methods are helpful when it comes to helping companies with customer segmentation. Customer targeting is the process of analyzing customer features to select those customers who are more prone to a target product or service. By making intelligent use of data, companies could make a big difference to their competitors. 

Advanced analytics plays a key role when it comes to selecting potentially profitable clients, which allows the design of more effective marketing campaigns. By using the four steps of advanced analytics: descriptive, diagnostic, predictive, and prescriptive, we would be able to answer key questions such as "what happened?", "why did it happen?", "what will happen?", and "how can we make it happen?" 

In this report, we would be covering most of those steps. Our primary goal is to build a predictive model to answer a simple yes or no question: to determine whether a client will sign on to a long-term deposit. A model as such would allow banks to save on marketing expense on groups of customers that have a low chance of subscription, and focus on other customers that have a high chance of success. Overall, this would improve the profitability of banks and ultimately decrease marketing deficiencies.

While our main goal is to build a classification model and assist with bank marketing efforts, we would also like to conduct an exploratory data analysis (EDA) to explore relationships between different input variables. We would report any useful insights along the way, which covers both the "descriptive" and "diagnostic" parts of the four steps of advanced analytics as mentioned in the article.

*[0] Lopez, R. (2020). Customer segmentation using machine learning. Retrieved December 12, 2020, from https://www.neuraldesigner.com/blog/customer_segmentation_using_advanced_analytics*

## 1.2 Statistical Questions of Interest

To answer the primary scientific question of interest, we would fit our model in 2 different methods. The response will be a binary yes/no variable "has the client subscribed a term deposit?" All other variables provided will then be our input variables to allow us to build this model. Here, our 2  classification methods are

1. Logistic Regression
2. Random Forest

We would then use both backward and forward stepwise model selection using a likelihood ratio test (LRT) to conduct a heauristic model selection and prune down our model. We would also use AIC and BIC and compare the results of those with LRT. We would also use cross validation (CV) to obtain more robust results. Lastly we will choose between the two and select the better one using with better accuracy to be our final candidate.

# 2. Analysis Plan
We would first start by conducting an Exploratory Categorical Data Analysis (EDA) to evaluate the relationship between different variables to the response variable. This would provide a descriptive story as to answering the most basic question like "what happened?" Then, we would start building our classification model using two methods.

## 2.1 Population and Study Design
The target population of this analysis would be all customers of banks in Portugal. We believe that sample from this particular Portuguese bank would be representative of all banks in Portugal. To Build the models we are using a dataset with 50% of the data. The other 50% will be used to for testing the accuracy of the models. This will help us assess which model is better at categorizing the data.

## 2.2 Statistical Analysis

### 2.2.1 Descriptive Analysis

```{r}
summary(bank.data)
deposit.status = bank.data$y 
table(bank.data$housing, bank.data$y)

#Age
ggplot(bank.data,aes(x=bank.data$age,fill=deposit.status)) + geom_histogram(binwidth=1) +
  labs(y= "Number of Clients", x="Age", title = "Distribution of Deposits by Age")
age.desc = bank.data %>% group_by(y) %>% summarise(age.mean = mean(age), .groups = 'drop')

#Job
ggplot(bank.data, aes(x=bank.data$job,fill=deposit.status)) + geom_bar(position = position_dodge())+
  labs(y= "Number of Clients", x="Job", title = "Distribution of Deposits by Job Type")+
  theme(axis.text.x = element_text(size = 10, angle = 45, hjust=1, vjust=1))

#Marital Status
ggplot(bank.data, aes(x=bank.data$marital,fill=deposit.status)) + geom_bar(position = position_dodge()) +
  labs(y= "Number of Clients", x="Marital Status", title = "Distribution of Deposits by Marital Status")

#Education
ggplot(bank.data, aes(x=bank.data$education,fill=deposit.status)) + geom_bar(position = position_dodge())+
  labs(y= "Number of Clients", x="Education", title = "Distribution of Deposits by Educational Qualification")

#Credit Default
ggplot(bank.data, aes(x=bank.data$default,fill=deposit.status)) + geom_bar(position = position_dodge())+
  labs(y= "Number of Clients", x="Credit Default", title = "Distribution of Credit Default and Deposits")

#Housing Loan
ggplot(bank.data, aes(x=bank.data$housing,fill=deposit.status)) + geom_bar(position = position_dodge())+
  labs(y= "Number of Clients", x="Contact", title = "Distribution of Client Having a Housing Loan and Deposit")

#Contact
ggplot(bank.data, aes(x=bank.data$contact,fill=deposit.status)) + geom_bar(position = position_dodge())+
  labs(y= "Number of Clients", x="Contact", title = "Distribution of Deposits by Contact")

#Loans
ggplot(bank.data, aes(x=bank.data$loan,fill=deposit.status)) + geom_bar(position = position_dodge())+
  labs(y= "Number of Clients", x="Loan", title = "Distribution of Clients with Loans and Deposit")

#month
ggplot(bank.data, aes(x=bank.data$month,fill=deposit.status)) + geom_bar(position = position_dodge())+
  labs(y= "Number of Clients", x="Month", title = "Distribution of Deposits by Month")

#Day
ggplot(bank.data, aes(x=bank.data$day,fill=deposit.status)) + geom_bar(position = position_dodge())+
  labs(y= "Number of Clients", x="Day of Week", title = "Distribution of Deposits by Day of Week")

#Previous Campaign Outcome
ggplot(bank.data, aes(x=bank.data$poutcome,fill=deposit.status)) + geom_bar(position = position_dodge()) +
  labs(y= "Number of Clients", x="poutcome", title = "Previous Campaign Outcome")

##Interactive EDA (NOTE: SET TARGET TO y. use install.packages("explore"))
explore_shiny(bank.data)
```
(Note: Stop explore_shiny manually to proceed with remaining code)

A short summary of the Exploratory Data Analysis is as follows:

1.) Mean Age: 40.936
    Median Age: 39
    25th Percentile Age: 33
    75th Percentile Age: 48
    As we can see in the interactive plots when split by target is deselected, we can see that customers who are over the age of 60 are responsible for 33.6% of all deposits made followed by customers under the age of 30 at 18.5%

2.) Jobs: 
We can see a distribution of the types of jobs that dataset contains:
     admin.        = 5 171 (11.4%)
     blue-collar   = 9 732 (21.5%)
     entrepreneur  = 1 487 (3.3%)
     housemaid     = 1 240 (2.7%)
     management    = 9 458 (20.9%)
     retired       = 2 264 (5%)
     self-employed = 1 579 (3.5%)
     services      = 4 154 (9.2%)
     student       = 938 (2.1%)
     technician    = 7 597 (16.8%)
Most of the deposits are made by customers who work in management. Nearly 25% of customers who work as managers deposit money with the bank, followed by techinicians at around 17%.

3.) Marital Status:
Distribution of the martial status of customers is as follows:
     divorced = 5 207 (11.5%)
     married  = 27 214 (60.2%)
     single   = 12 790 (28.3%)
Married couples who deposit money in the bank account for 61.3% of deposits made, followed by 36.2% of single customers making deposits, followed by 11.8% of divorced customers make deposits. 

4.) Education: 
Summary statistics for educational distribution of the bank's customers are as follows:
     primary = 6 851 (15.2%)
     secondary = 23 202 (51.3%)
     tertiary  = 13 301 (29.4%)
     unknown   = 1 857 (4.1%)


5.) Defaults:
98.2% of customers do not default on credit payments. 11.8% of customers who do not default on payments make deposits as compared to 6.4% of customers who default on credit. 

6.) Balance:
Summary statistics of balance is as follows:
  Minimum balance: -8019
  Maximum balance: 102,127
  25th percentile balance: 72
  75th percentile balance: 1428
  median balance: 448
  mean: 1362
From the distribution plot we see that customers with a balance of 3000-6000 account for most customers who deposit money with the bank. 

7.) Housing Loan
55.6% of the data do have housing loans, which leaves the remaining 44.4% of the data that does not have housing loans. Of those who had a housing loan, Only 36.6% have deposited with the bank, while of those who did not have a housing loan 63.4% have deposited with the bank. Looking at this we can say that those without a
housing loan are more likely to deposit with the bank from the marketing channels.

8.)



### 2.2.2 Main Analysis

To answer our question of interest, building a predictive categorial model, we choose to utilize two different methods. 

1. Logistic regression, particularly using different stepwise methods (backward, forward, both) and different selection criteria (Likelihood-Ratio Test, AIC, BIC) to conduct variable selection for building the most effective model.

2. Random Forest

Finally, we will compare the two and see how the models compare to each other, and select a ultimate winner between the two. 


# 3. Extra-Credit Methods

## First Method: Stepwise (Forward, Backward, Both) Variable Selection using AIC, BIC, and Likelihood Ratio Test (LRT) for our Logistic Regression Model. Specifically, we have computed results for: 

  1. LRT Forward - at 5% significance level, dropped variables *age*, *default*, *pdays*, *previous*, and *balance*.
  
  2. LRT Backward - same dropped variables as above.
  
  3. AIC Forward - dropped variables *previous*, *pdays*, *default*, *age*.
  
  4. AIC Backward - same dropped variables as above. 
  
  5. AIC Both - same dropped variables as above.
  
  6. BIC Forward - dropped variables *education*, *balance*, *previous*, *default*, *age*, *pdays*, and *job*
  
  7. BIC Backward - same dropped variables as above.
  
  8. BIC Both - same dropped variables as above.
  
<span style="color:red"> Note, we tried adding interaction variables but it was too computationally complicated for R and our dataset </span>
Ultimately, we see that all all 3 stepwise selections (backward, forward, both) using AIC yield the exact same model. We have dropped pdays, default, age, and previous. Our final logistic regression model would be:

*y ~ duration + poutcome + month + contact + housing + job + campaign + loan + marital + day + education + balance*

The reason we think AIC is the most robust and appropriate and do not use LRT nor BIC is because firstly, a Likelihood Ratio Test only provides a "heauristic model selection," it's main purpose is not for variable selection but just a quick and convinient way of selecting variables. We chose to select variables at a 5% significance level, but this selection is in fact trivial. If we chose a different alpha level such as 1% or 10%, our variables would be different. Therefore, it is an inferior choice as compared to AIC and BIC.

While AIC and BIC are both robust parameters, for the purposes of answering this statistical question of interest, we choose AIC over BIC. BIC usually penalizes complicated models more than AIC. Naturally, AIC would create a more complicated model (with more predictors) yet it is actually better for prediction. The Akaike information criterion is known to be efficient in the sense that its prediction performance is asymptotically equivalent to the best offered by the candidate models (Ding, Tarokh, &amp; Yang, Bridging AIC and BIC: a new criterion for autoregression 2016)[1]. When the true model is not in the candidate model set the AIC is effcient, in that it will asymptotically choose whichever model minimizes the mean squared error of prediction/estimation. The BIC is not effcient under these circumstances (Vrieze, Model selection and psychological theory: a discussion of the differences between the Akaike information criterion (AIC) and the Bayesian information criterion (BIC) 2012) [2]. Models built using BIC would be less complicated, which would be betteer for inference, but worse than AIC in terms of predictive performance. 

*[1] Ding, J., Tarokh, V., &amp; Yang, Y. (2016, August 24). Bridging AIC and BIC: A new criterion for autoregression. Retrieved December 13, 2020, from https://arxiv.org/abs/1508.02473*

*[2] Vrieze, S. (2012, June). Model selection and psychological theory: A discussion of the differences between the Akaike information criterion (AIC) and the Bayesian information criterion (BIC). Retrieved December 13, 2020, from https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3366160/*

## Second Method: 

# 4. Results
## Method 1. Logistic Regression & Model Selection

Here is a summary of a logistic regression model with all the input variables: 
```{r}
#make copy of data
lr.bank.data <- bank.train

#transform y into 1 and 0
lr.bank.data$y <- ifelse(lr.bank.data$y=='yes',1,0)

#create logistic model with all variables
logistic.model <- glm(as.factor(y)~., binomial(link = "logit"),lr.bank.data)

#summary of logistic model
summary(logistic.model)
```


### Backward Stepwise Model Selection Using LRT
```{r, message=FALSE, warning = FALSE, echo = FALSE}
#perform feature selection using likelihood ratio test (comparing a certain coeffeicient vs. it to be zero) to prune down model. Alpha = 0.05
b_first_run <- drop1(glm(as.factor(y)~., binomial,lr.bank.data),test="LRT")
#Find largest p value (larger p value indicates insignificance) to be age, so we drop age from our second run
b_second_run <- drop1(glm(as.factor(y)~job+marital+education+default+balance+housing+loan+contact+day+month+duration+campaign+pdays+previous+poutcome, binomial,lr.bank.data),test="LRT")
#Find largest p value to be default, so drop default from our third run
b_third_run <- drop1(glm(as.factor(y)~job+marital+education+balance+housing+loan+contact+day+month+duration+campaign+pdays+previous+poutcome, binomial,lr.bank.data),test="LRT")
#Find largest p value to be pdays, so drop pdays from our fourth run
b_fourth_run <- drop1(glm(as.factor(y)~job+marital+education+balance+housing+loan+contact+day+month+duration+campaign+previous+poutcome, binomial,lr.bank.data),test="LRT")
#Find largest p value to be previous, so drop previous from our fifth run
b_fifth_run <- drop1(glm(as.factor(y)~job+marital+education+balance+housing+loan+contact+day+month+duration+campaign+poutcome, binomial,lr.bank.data),test="LRT")
#Find largest p value to be balance, so drop balance from our sixth run
b_sixth_run <- drop1(glm(as.factor(y)~job+marital+education+housing+loan+contact+day+month+duration+campaign+poutcome, binomial,lr.bank.data),test="LRT")
#Stop. All p values significant at alpha = 0.05. Drawback = cutoff level is trivial choice. 
#Final mode: (y) ~ job + marital + education + housing + loan + contact + day + month + duration + campaign + poutcome
#Dropped age, default, pdays, previous, balance.
```
Perform feature selection using likelihood ratio test (comparing a certain coeffeicient vs. it to be zero) to prune down model at alpha = 0.05. We find the largest p value at each step to drop the variable. After 6 steps, we have dropped age, default, previous, and balance from our model. All variables at the sixth run have p value significant at 5% significance level, so we stop our test. The final model is (y) ~ job + marital + education + housing + loan + contact + day + month + duration + campaign + poutcome. 


### Forward Stepwise Model Selection Using LRT
```{r}
#perform feature selection using likelihood ratio test (comparing a certain coeffeicient vs. it to be zero) to prune model. Forward stepwise model selection adds variable with smallest p value each time. Alpha = 0.05
f_first_run <- add1(glm(as.factor(y)~1, binomial, lr.bank.data),
     scope = ~age+job+marital+education+default+balance+housing+loan+contact+day+month+duration+campaign+pdays+previous+poutcome,
     test = "LRT")
#Find smallest p value (small p value indicates significance) to be duration, so we add duration to our second run
f_second_run <- add1(glm(as.factor(y)~duration, binomial, data = lr.bank.data),
     scope = ~.+age+job+marital+education+default+balance+housing+loan+contact+day+month+campaign+pdays+previous+poutcome,
     test = "LRT")
#Find smallest p value (small p value indicates significance) to be poutcome, so we add poutcome to our third run
f_third_run <- add1(glm(as.factor(y)~duration+poutcome, binomial, data = lr.bank.data),
     scope = ~.+age+job+marital+education+default+balance+housing+loan+contact+day+month+campaign+pdays+previous,
     test = "LRT")
#Find smallest p value (small p value indicates significance) to be month, so we add month to our fourth run
f_fourth_run <- add1(glm(as.factor(y)~duration+poutcome+month, binomial, data = lr.bank.data),
     scope = ~.+age+job+marital+education+default+balance+housing+loan+contact+day+campaign+pdays+previous,
     test = "LRT")
#Find smallest p value (small p value indicates significance) to be contact, so we add contact to our fifth run
f_fifth_run <- add1(glm(as.factor(y)~duration+poutcome+month+contact, binomial, data = lr.bank.data),
     scope = ~.+age+job+marital+education+default+balance+housing+loan+day+campaign+pdays+previous,
     test = "LRT")
#Find smallest p value (small p value indicates significance) to be housing, so we add housing to our sixth run
f_sixth_run <- add1(glm(as.factor(y)~duration+poutcome+month+contact+housing, binomial, data = lr.bank.data),
     scope = ~.+age+job+marital+education+default+balance+loan+day+campaign+pdays+previous,
     test = "LRT")
#Find smallest p value (small p value indicates significance) to be job, so we add job to our seventh run
f_seventh_run <- add1(glm(as.factor(y)~duration+poutcome+month+contact+housing+job, binomial, data = lr.bank.data),
     scope = ~.+age+marital+education+default+balance+loan+day+campaign+pdays+previous,
     test = "LRT")
#Find smallest p value (small p value indicates significance) to be campaign, so we add campaign to our eighth run
f_eighth_run <- add1(glm(as.factor(y)~duration+poutcome+month+contact+housing+job+campaign, binomial, data = lr.bank.data),
     scope = ~.+age+marital+education+default+balance+loan+day+pdays+previous,
     test = "LRT")
#Find smallest p value (small p value indicates significance) to be loan, so we add loan to our ninth run
f_ninth_run <- add1(glm(as.factor(y)~duration+poutcome+month+contact+housing+job+campaign+loan, binomial, data = lr.bank.data),
     scope = ~.+age+marital+education+default+balance+day+pdays+previous,
     test = "LRT")
#Find smallest p value (small p value indicates significance) to be marital, so we add marital to our tenth run
f_tenth_run <- add1(glm(as.factor(y)~duration+poutcome+month+contact+housing+job+campaign+loan+marital, binomial, data = lr.bank.data),
     scope = ~.+age+education+default+balance+day+pdays+previous,
     test = "LRT")
#Find smallest p value (small p value indicates significance) to be education, so we add education to our eleventh run
f_eleventh_run <- add1(glm(as.factor(y)~duration+poutcome+month+contact+housing+job+campaign+loan+marital+education, binomial, data = lr.bank.data),
     scope = ~.+age+default+balance+day+pdays+previous,
     test = "LRT")
#Find smallest p value (small p value indicates significance) to be day, so we add day to our twelve run
f_twelve_run <- add1(glm(as.factor(y)~duration+poutcome+month+contact+housing+job+campaign+loan+marital+education+day, binomial, data = lr.bank.data),
     scope = ~.+age+default+balance+pdays+previous,
     test = "LRT")
#Stop. All p values significant at alpha = 0.05. Drawback = cutoff level is trivial choice. 
#Final mode: y ~ job + marital + education + housing + loan + contact + day + month + duration + campaign + poutcome
#Dropped age, default, pdays, previous, balance.
```

Perform feature selection using likelihood ratio test (comparing a certain coeffeicient vs. it to be zero) to prune down model at alpha = 0.05. We find the smallest p value at each step to add the variable. After 12 steps, we have added job, marital, education, housing, loan, contact, day, month, duration, campaign, and poutcome to our model. All variables at the twelveth run have p value insignificant at 5% significance level, so we stop our test. The final model is (y) ~ job + marital + education + housing + loan + contact + day + month + duration + campaign + poutcome. We have left out age, default, pdays, previous, and balance. 

Not surprisingly, backward and forward stepwise using LRT yield same results. We have excluded age, default, balance, pdays, and previous from our model, and the model selected from LRT is 

(y) ~ job + marital + education + housing + loan + contact + day + month + duration + campaign + poutcome. 

### Forward Selection Using AIC - (Less penalize on larger models, so creates more complicated model, but better predictive performance)
```{r, message=FALSE, warning = FALSE}
f_AIC <- step(glm(y~1, binomial, lr.bank.data),
     scope = ~age+job+marital+education+default+balance+housing+loan+contact+day+month+duration+campaign+pdays+previous+poutcome,
     direction = "forward")
f_AIC
#previous, pdays, default, age
# y ~ duration + poutcome + month + contact + housing + job + campaign + loan + marital + education + day + balance
```
According to this criteria, we excluded previous, pdays, default, and age. Our model is y ~ duration + poutcome + month + contact + housing + job + campaign + loan + marital + education + day + balance. 

### Forward Selection Using BIC - (More penalize on larger models, builds simpler models, less complicated and better for inference)
```{r, message=FALSE, warning = FALSE, echo = FALSE}
f_BIC <- step(glm(y~1, binomial, lr.bank.data),
     scope = ~age+job+marital+education+default+balance+housing+loan+contact+day+month+duration+campaign+pdays+previous+poutcome,
     direction = "forward",
     k = log(dim(lr.bank.data)[1]))
f_BIC
#education, balance, previous, default, age, pdays, job
#y ~ duration + poutcome + month + contact + housing + campaign + marital + loan + day
```
According to this criteria, we excluded education, balance, previous, default, age, pdays, and job. Our model is y ~ duration + poutcome + month + contact + housing + campaign + marital + loan + day.

### Backward Selection Using AIC
```{r, message=FALSE, warning = FALSE, echo = FALSE}
b_AIC <- step(glm(y~., binomial, lr.bank.data),
     direction = "backward")
#pdays, default, age, previous
# y ~ job + marital + education + balance + housing + loan + contact + day + month + duration + campaign + poutcome
b_AIC
```
According to this criteria, we excluded previous, pdays, default, and age. Our model is y ~ duration + poutcome + month + contact + housing + job + campaign + loan + marital + education + day + balance. Which is the same as Forward Selection using AIC.

### Backward Selection Using BIC
```{r, message=FALSE, warning = FALSE, echo = FALSE}
b_BIC <- step(glm(y~., binomial, lr.bank.data),
     direction = "backward",
     k = log(dim(lr.bank.data)[1]))
#education, balance, previous, default, age, pdays, job
#y ~ marital + housing + loan + contact + day + month + duration + campaign + poutcome
```
According to this criteria, we excluded education, balance, previous, default, age, pdays, and job. Our model is y ~ duration + poutcome + month + contact + housing + campaign + marital + loan + day. Which is the same as Forward Selection using BIC. 

### Both Directions Using AIC
```{r, message=FALSE, warning = FALSE, echo = FALSE}
both_AIC <- step(glm(y~1, binomial, lr.bank.data),
     scope = ~age+job+marital+education+default+balance+housing+loan+contact+day+month+duration+campaign+pdays+previous+poutcome,
     direction = "both")
both_AIC
#pdays, default, age, previous
#y ~ duration + poutcome + month + contact + housing + job + campaign + loan + marital + day + education + balance
```
Here, we exclude pdays, default, age, and previous. Our model is y ~ duration + poutcome + month + contact + housing + job + campaign + loan + marital + day + education + balance. 

### Both Directions Using BIC
```{r, message=FALSE, warning = FALSE, echo = FALSE}
both_BIC <- step(glm(y~1, binomial, lr.bank.data),
     scope = ~age+job+marital+education+default+balance+housing+loan+contact+day+month+duration+campaign+pdays+previous+poutcome,
     direction = "both",
     k = log(dim(lr.bank.data)[1]))
both_BIC
#y ~ duration + poutcome + month + contact + housing + campaign + marital + loan + day
#education, balance, previous, default, age, pdays, job
```
Here, we exclude education, balance, previous, default, age, pdays, and job. Our model is y ~ duration + poutcome + month + contact + housing + campaign + marital + loan + day. 

### Building a Confusion Matrix with our final logistic regression model
Again, our final model is the one with variables selected from AIC. 

y ~ duration + poutcome + month + contact + housing + job + campaign + loan + marital + day + education + balance

The confusion matrix and our classification accuracy with this model are below.

```{r}
#Build logistic model with selected significant variables using AIC stepwise variable selection
logistic.model.AIC <- glm(y ~ duration + poutcome + month + contact + housing + job + campaign + loan + marital + day + education + balance, binomial(link = "logit"),lr.bank.data)

pred = format(round(predict(logistic.model.AIC, newdata = bank.test, type = "response")))
conf = table(pred, as.factor(bank.test$y));conf

#Tuning parameters
pred2 = predict(logistic.model.AIC, newdata = bank.test, type = "response")
pred.tune.log = prediction(pred2, bank.test$y)
perf.tune.log = performance(pred.tune.log, measure = "tpr", x.measure = "fpr")
accuracy.log = performance(pred.tune.log, measure = "acc", x.measure = "cutoff")
plot(accuracy.log)
plot(perf.tune.log) ##selecting default threshold value of 0.5
auc.log = as.numeric(performance(pred.tune.log, measure = "auc")@y.values)
auc.log
roc = performance

#Classification accuracy
(conf[1,1]+conf[2,2])/(sum(conf))
```

### Correlation Analysis

```{r, message=FALSE, warning = FALSE}
bank.num = data.frame(as.numeric(as.factor(bank.data$age)),
                      as.numeric(as.factor(bank.data$job)),
                      as.numeric(as.factor(bank.data$marital)),
                      as.numeric(as.factor(bank.data$education)),
                      as.numeric(as.factor(bank.data$default)),
                      as.numeric(as.factor(bank.data$balance)),
                      as.numeric(as.factor(bank.data$housing)),
                      as.numeric(as.factor(bank.data$loan)),
                      as.numeric(as.factor(bank.data$contact)),
                      as.numeric(as.factor(bank.data$day)),
                      as.numeric(as.factor(bank.data$month)),
                      as.numeric(as.factor(bank.data$duration)),
                      as.numeric(as.factor(bank.data$campaign)),
                      as.numeric(as.factor(bank.data$pdays)),
                      as.numeric(as.factor(bank.data$previous)),
                      as.numeric(as.factor(bank.data$poutcome)),
                      as.numeric(as.factor(bank.data$y)))
                      
colnames(bank.num) = c("Age", "Job", "Marital", "Education", "Default", "Balance", "Housing", " Loan", "Contact", "Day", "Month", "Duration","Campaign", "Pdays", "Previous", "Poutcome","Y")



bank.num %>%
  cor() %>%
  corrplot(method = "number",
           tl.srt = 45,
           bg = "black",
           order = "FPC",
           tl.col = "black",
           number.cex = 0.5)
```
The above correlogram is ordered in a first principle component manner. We can see that for the response variable, i.e. Y, the following can be said for its correlation with respect to the predictor variables:
1.) 

# Method 2. Random Forest
The second method that we choose is Random Forest. A Random Forest is a collection of decision trees and average/majority vote of the forest is selected as the predicted output. The reason that we choose it is because a Random Forest model will be less prone to over fitting than a decision tree, and gives a more generalized solution. Random Forest is more robust and accurate than decision trees.

Particularly, we would first investigate the importance of variables and choose sqrt(p) as our a rule of thumb for amount of predictors. We would then use mtry from 1 to p and then use CV. 

```{r, message=FALSE,warning=FALSE, echo = FALSE}
#create random forest model with all variables, mtry = sqrt(p) for classification
rf <- randomForest(as.factor(y)~ ., data = bank.train, importance = TRUE, mtry = sqrt(dim(bank.train)[2]-1), ntree = 300)
#importance of variables
importance_of_variables <- importance(rf)
#plot of importance of variables
varImpPlot(rf)

#duration is the most important variable, default is the least important

###################################################################
#oob.err = double(16)                       #Out-of-bag error
#test.err = double(16)                      #Test error
#for(mtry in 1:16){
#  fit = randomForest(as.factor(y)~., data = bank.train, mtry=mtry, ntree = 1)
#  oob.err[mtry] = fit$err.rate[1]
#  pred = predict(fit, boston[-train,])
#  test.err[mtry] = with(boston[-train,], mean( (medv-pred)^2 ))
#}
###################################################################
#matplot(1:mtry, cbind(test.err, oob.err), pch = 23, col = c("red", "blue"), type = "b", ylab="Mean Squared Error",xlab="m")
#legend("topright", legend = c("OOB", "Test"), pch = 23, col = c("red", "blue"))
###############################################################

prediction_rf <- format(predict(rf, newdata = bank.test))
conf_rf <- table(prediction_rf, as.factor(bank.test$y))
conf_rf


pred3 = predict(rf, newdata = bank.test, type = "response")
pred.tune.rf = prediction(pred3, bank.test$y)
perf.tune.rf = performance(pred.tune.rf, measure = "tpr", x.measure = "fpr")
accuracy.rf = performance(pred.tune.rf, measure = "acc", x.measure = "cutoff")
acc.rf = as.numeric(performance(pred.tune.rf, measure = "acc")@y.values)
acc.rf #accuracy
plot(accuracy.rf)
plot(perf.tune.rf) 
auc.rf = as.numeric(performance(pred.tune.rf, measure = "auc")@y.values)
auc.rf #auc

#Classification accuracy
(conf[1,1]+conf[2,2])/(sum(conf))
###
#Classification accuracy
classicitation_accuracy_rf <- (conf_rf[1,1]+conf_rf[2,2])/(sum(conf_rf))
```
We see that in both cases, duration is the most important variable, whereas default is the least important variable. Additionally, the classification accuracy according to our model is `r classicitation_accuracy_rf`, very close to what we get in our logistic model after AIC variable selection. 


# Code Appendix
```{r, ref.label=knitr::all_labels(),echo=TRUE,eval=FALSE}
```