---
title: "STA141A Final Project"
author: "Animay Sharma, Aditya Kallepalli, Charles Chien, Shaumik Pathak"
date: "12/14/2020"
output: pdf_document
---

# 1. Introduction
## 1.1 Background
Here, we have a set of marketing data of a banking institution. The data is related with direct marketing campaigns of a Portuguese banking institution. The marketing campaigns were based on phone calls. Often, more than one contact to the same client was required, in order to access if the product (bank term deposit) would be ('yes') or not ('no') subscribed.

Among the four datasets provided, we chose to utilize the "bank-full.csv" in this report, with 17 different inputs.

As mentioned in article (Lopez, Customer segmentation using machine learning 2020) [1], data science and machine learning methods are helpful when it comes to helping companies with customer segmentation. Customer targeting is the process of analyzing customer features to select those customers who are more prone to a target product or service. By making intelligent use of data, companies could make a big difference to their competitors. 

Advanced analytics plays a key role when it comes to selecting potentially profitable clients, which allows the design of more effective marketing campaigns. By using the four steps of advanced analytics: descriptive, diagnostic, predictive, and prescriptive, we would be able to answer key questions such as "what happened?", "why did it happen?", "what will happen?", and "how can we make it happen?" 

In this report, we would be covering most of those steps. Our primary goal is to build a predictive model to answer a simple yes or no question: to determine whether a client will sign on to a long-term deposit. A model as such would allow banks to save on marketing expense on groups of customers that have a low chance of subscription, and focus on other customers that have a high chance of success. Overall, this would improve the profitability of banks and ultimately decrease marketing deficiencies.

While our main goal is to build a classification model and assist with bank marketing efforts, we would also like to conduct an exploratory data analysis (EDA) to explore relationships between different input variables. We would report any useful insights along the way, which covers both the "descriptive" and "diagnostic" parts of the four steps of advanced analytics as mentioned in the article.

*[1] Lopez, R. (2020). Customer segmentation using machine learning. Retrieved December 12, 2020, from https://www.neuraldesigner.com/blog/customer_segmentation_using_advanced_analytics*

## 1.2 Statistical Questions of Interest

To answer the primary scientific question of interest, we would fit our model in 2 different methods. The response will be a binary yes/no variable "has the client subscribed a term deposit?" All other variables provided will then be our input variables to allow us to build this model. Here, our 2  classification methods are

1. Logistic Regression
2. Random Forest

We would then use both backward and forward stepwise model selection using a likelihood ratio test (LRT) to conduct a heauristic model selection and prune down our model. We would also use cross validation (CV) to obtain more robust results.

# 1.) Setup
```{r, message=FALSE, warning = FALSE}
set.seed(1984)
library(readr)
library(tidyverse)
library(fastDummies)
library(knitr)
library(plyr)
library(dplyr)
library(explore)
library(corrplot)
library(ROCR)
library(cutpointr)
library(caret)

#import dataset
bank.data <- read_delim("datasets/bank-full.csv",";", escape_double = FALSE, trim_ws = TRUE)
#clean NA vaules
bank.data = na.omit(bank.data)
#look at sample of our dataset
head(bank.data)
```

## modifying data set
```{r}
cat_data = data.frame(bank.data$job,bank.data$marital,bank.data$education)
bin_cat_data = dummy_cols(cat_data)
bin_cat_data = bin_cat_data %>% select(4:22)

yesno_data = data.frame(bank.data$default,bank.data$housing,bank.data$loan,bank.data$y)
yesno_data$bank.data.default <- revalue(yesno_data$bank.data.default, c("yes"=1))
yesno_data$bank.data.default <- revalue(yesno_data$bank.data.default, c("no"=0))
yesno_data$bank.data.housing <- revalue(yesno_data$bank.data.housing, c("yes"=1))
yesno_data$bank.data.housing <- revalue(yesno_data$bank.data.housing, c("no"=0))
yesno_data$bank.data.loan <- revalue(yesno_data$bank.data.loan, c("yes"=1))
yesno_data$bank.data.loan <- revalue(yesno_data$bank.data.loan, c("no"=0))
yesno_data$bank.data.y <- revalue(yesno_data$bank.data.y, c("yes"=1))
yesno_data$bank.data.y <- revalue(yesno_data$bank.data.y, c("no"=0))
remaining_data = bank.data %>% select(1,6,9,10,11,12,13,14,15,16)
master_bin_data = cbind(bin_cat_data,yesno_data,remaining_data)
head(master_bin_data)
```


## 2.) Exploratory Categorical Data Analysis:
```{r}
summary(bank.data)
deposit.status = bank.data$y         

#Age
ggplot(bank.data,aes(x=bank.data$age,fill=deposit.status)) + geom_histogram(binwidth=1) +
  labs(y= "Number of Clients", x="Age", title = "Distribution of Deposits by Age")
age.desc = bank.data %>% group_by(y) %>% summarise(age.mean = mean(age), .groups = 'drop')

#Job
ggplot(bank.data, aes(x=bank.data$job,fill=deposit.status)) + geom_bar(position = position_dodge())+
  labs(y= "Number of Clients", x="Job", title = "Distribution of Deposits by Job Type")+
  theme(axis.text.x = element_text(size = 10, angle = 45, hjust=1, vjust=1))

#Marital Status
ggplot(bank.data, aes(x=bank.data$marital,fill=deposit.status)) + geom_bar(position = position_dodge()) +
  labs(y= "Number of Clients", x="Marital Status", title = "Distribution of Deposits by Marital Status")

#Education
ggplot(bank.data, aes(x=bank.data$education,fill=deposit.status)) + geom_bar(position = position_dodge())+
  labs(y= "Number of Clients", x="Education", title = "Distribution of Deposits by Educational Qualification")

#Credit Default
ggplot(bank.data, aes(x=bank.data$default,fill=deposit.status)) + geom_bar(position = position_dodge())+
  labs(y= "Number of Clients", x="Credit Default", title = "Distribution of Credit Default and Deposits")

#Housing Loan
ggplot(bank.data, aes(x=bank.data$housing,fill=deposit.status)) + geom_bar(position = position_dodge())+
  labs(y= "Number of Clients", x="Contact", title = "Distribution of Client Having a Housing Loan and Deposit")

#Contact
ggplot(bank.data, aes(x=bank.data$contact,fill=deposit.status)) + geom_bar(position = position_dodge())+
  labs(y= "Number of Clients", x="Contact", title = "Distribution of Deposits by Contact")

#Loans
ggplot(bank.data, aes(x=bank.data$loan,fill=deposit.status)) + geom_bar(position = position_dodge())+
  labs(y= "Number of Clients", x="Loan", title = "Distribution of Clients with Loans and Deposit")

#month
ggplot(bank.data, aes(x=bank.data$month,fill=deposit.status)) + geom_bar(position = position_dodge())+
  labs(y= "Number of Clients", x="Month", title = "Distribution of Deposits by Month")

#Day
ggplot(bank.data, aes(x=bank.data$day,fill=deposit.status)) + geom_bar(position = position_dodge())+
  labs(y= "Number of Clients", x="Day of Week", title = "Distribution of Deposits by Day of Week")

#Previous Campaign Outcome
ggplot(bank.data, aes(x=bank.data$poutcome,fill=deposit.status)) + geom_bar(position = position_dodge()) +
  labs(y= "Number of Clients", x="poutcome", title = "Previous Campaign Outcome")

##Interactive EDA (NOTE: SET TARGET TO y. use install.packages("explore"))
explore_shiny(bank.data)

```
A short summary of the Exploratory Data Analysis is as follows:

1.) Mean Age: 40.936
    Median Age: 39
    25th Percentile Age: 33
    75th Percentile Age: 48
    As we can see in the interactive plots when split by target is deselected, we can see that customers who are over the age of 60 are responsible for 33.6% of all deposits made followed by customers under the age of 30 at 18.5%

2.) Jobs: 
We can see a distribution of the types of jobs that dataset contains:
     admin.        = 5 171 (11.4%)
     blue-collar   = 9 732 (21.5%)
     entrepreneur  = 1 487 (3.3%)
     housemaid     = 1 240 (2.7%)
     management    = 9 458 (20.9%)
     retired       = 2 264 (5%)
     self-employed = 1 579 (3.5%)
     services      = 4 154 (9.2%)
     student       = 938 (2.1%)
     technician    = 7 597 (16.8%)
Most of the deposits are made by customers who work in management. Nearly 25% of customers who work as managers deposit money with the bank, followed by techinicians at around 17%.

3.) Marital Status:
Distribution of the martial status of customers is as follows:
     divorced = 5 207 (11.5%)
     married  = 27 214 (60.2%)
     single   = 12 790 (28.3%)
Married couples who deposit money in the bank account for 61.3% of deposits made, followed by 36.2% of single customers making deposits, followed by 11.8% of divorced customers make deposits. 

4.) Education: 
Summary statistics for educational distribution of the bank's customers are as follows:
     primary = 6 851 (15.2%)
     secondary = 23 202 (51.3%)
     tertiary  = 13 301 (29.4%)
     unknown   = 1 857 (4.1%)


5.) Defaults:
98.2% of customers do not default on credit payments. 11.8% of customers who do not default on payments make deposits as compared to 6.4% of customers who default on credit. 

5.) Balance:
Summary statistics of balance is as follows:
  Minimum balance: -8019
  Maximum balance: 102,127
  25th percentile balance: 72
  75th percentile balance: 1428
  median balance: 448
  mean: 1362
From the distribution plot we see that customers with a balance of 3000-6000 account for most customers who deposit money with the bank. 

6.) Housing Loan

#test and train dataset
```{r, message=FALSE, warning = FALSE}
bank.train = bank.data %>%
  sample_frac(0.5)

bank.test = bank.data %>%
  setdiff(bank.train)

bank.test$y <- ifelse(bank.test$y=='yes',1,0)
```

# Logistic regression & Model Selection - Charles
```{r, message=FALSE, warning = FALSE}
#make copy of data
lr.bank.data <- bank.train

#transform y into 1 and 0
lr.bank.data$y <- ifelse(lr.bank.data$y=='yes',1,0)

#create logistic model with all variables
logistic.model <- glm(as.factor(y)~., binomial(link = "logit"),lr.bank.data)

#summary of logistic model
summary(logistic.model)
```

## Backward stepwise model selection, note tried adding interactions but stack overflow
```{r, message=FALSE, warning = FALSE}
#perform feature selection using likelihood ratio test (comparing a certain coeffeicient vs. it to be zero) to prune down model. Alpha = 0.05
b_first_run <- drop1(glm(as.factor(y)~., binomial,lr.bank.data),test="LRT")
#Find largest p value (larger p value indicates insignificance) to be age, so we drop age from our second run
b_second_run <- drop1(glm(as.factor(y)~job+marital+education+default+balance+housing+loan+contact+day+month+duration+campaign+pdays+previous+poutcome, binomial,lr.bank.data),test="LRT")
#Find largest p value to be default, so drop default from our third run
b_third_run <- drop1(glm(as.factor(y)~job+marital+education+balance+housing+loan+contact+day+month+duration+campaign+pdays+previous+poutcome, binomial,lr.bank.data),test="LRT")
#Find largest p value to be pdays, so drop pdays from our fourth run
b_fourth_run <- drop1(glm(as.factor(y)~job+marital+education+balance+housing+loan+contact+day+month+duration+campaign+previous+poutcome, binomial,lr.bank.data),test="LRT")
#Find largest p value to be previous, so drop previous from our fifth run
b_fifth_run <- drop1(glm(as.factor(y)~job+marital+education+balance+housing+loan+contact+day+month+duration+campaign+poutcome, binomial,lr.bank.data),test="LRT")
#Find largest p value to be balance, so drop balance from our sixth run
b_sixth_run <- drop1(glm(as.factor(y)~job+marital+education+housing+loan+contact+day+month+duration+campaign+poutcome, binomial,lr.bank.data),test="LRT")
#Stop. All p values significant at alpha = 0.05. Drawback = cutoff level is trivial choice. 
#Final mode: (y) ~ job + marital + education + housing + loan + contact + day + month + duration + campaign + poutcome
#Dropped age, default, pdays, previous, balance.
```


## Forward stepwise model selection, note tried adding interactions but stack overflow
```{r, message=FALSE, warning = FALSE}
#perform feature selection using likelihood ratio test (comparing a certain coeffeicient vs. it to be zero) to prune model. Forward stepwise model selection adds variable with smallest p value each time. Alpha = 0.05
f_first_run <- add1(glm(as.factor(y)~1, binomial, lr.bank.data),
     scope = ~age+job+marital+education+default+balance+housing+loan+contact+day+month+duration+campaign+pdays+previous+poutcome,
     test = "LRT")
#Find smallest p value (small p value indicates significance) to be duration, so we add duration to our second run
f_second_run <- add1(glm(as.factor(y)~duration, binomial, data = lr.bank.data),
     scope = ~.+age+job+marital+education+default+balance+housing+loan+contact+day+month+campaign+pdays+previous+poutcome,
     test = "LRT")
#Find smallest p value (small p value indicates significance) to be poutcome, so we add poutcome to our third run
f_third_run <- add1(glm(as.factor(y)~duration+poutcome, binomial, data = lr.bank.data),
     scope = ~.+age+job+marital+education+default+balance+housing+loan+contact+day+month+campaign+pdays+previous,
     test = "LRT")
#Find smallest p value (small p value indicates significance) to be month, so we add month to our fourth run
f_fourth_run <- add1(glm(as.factor(y)~duration+poutcome+month, binomial, data = lr.bank.data),
     scope = ~.+age+job+marital+education+default+balance+housing+loan+contact+day+campaign+pdays+previous,
     test = "LRT")
#Find smallest p value (small p value indicates significance) to be contact, so we add contact to our fifth run
f_fifth_run <- add1(glm(as.factor(y)~duration+poutcome+month+contact, binomial, data = lr.bank.data),
     scope = ~.+age+job+marital+education+default+balance+housing+loan+day+campaign+pdays+previous,
     test = "LRT")
#Find smallest p value (small p value indicates significance) to be housing, so we add housing to our sixth run
f_sixth_run <- add1(glm(as.factor(y)~duration+poutcome+month+contact+housing, binomial, data = lr.bank.data),
     scope = ~.+age+job+marital+education+default+balance+loan+day+campaign+pdays+previous,
     test = "LRT")
#Find smallest p value (small p value indicates significance) to be job, so we add job to our seventh run
f_seventh_run <- add1(glm(as.factor(y)~duration+poutcome+month+contact+housing+job, binomial, data = lr.bank.data),
     scope = ~.+age+marital+education+default+balance+loan+day+campaign+pdays+previous,
     test = "LRT")
#Find smallest p value (small p value indicates significance) to be campaign, so we add campaign to our eighth run
f_eighth_run <- add1(glm(as.factor(y)~duration+poutcome+month+contact+housing+job+campaign, binomial, data = lr.bank.data),
     scope = ~.+age+marital+education+default+balance+loan+day+pdays+previous,
     test = "LRT")
#Find smallest p value (small p value indicates significance) to be loan, so we add loan to our ninth run
f_ninth_run <- add1(glm(as.factor(y)~duration+poutcome+month+contact+housing+job+campaign+loan, binomial, data = lr.bank.data),
     scope = ~.+age+marital+education+default+balance+day+pdays+previous,
     test = "LRT")
#Find smallest p value (small p value indicates significance) to be marital, so we add marital to our tenth run
f_tenth_run <- add1(glm(as.factor(y)~duration+poutcome+month+contact+housing+job+campaign+loan+marital, binomial, data = lr.bank.data),
     scope = ~.+age+education+default+balance+day+pdays+previous,
     test = "LRT")
#Find smallest p value (small p value indicates significance) to be education, so we add education to our eleventh run
f_eleventh_run <- add1(glm(as.factor(y)~duration+poutcome+month+contact+housing+job+campaign+loan+marital+education, binomial, data = lr.bank.data),
     scope = ~.+age+default+balance+day+pdays+previous,
     test = "LRT")
#Find smallest p value (small p value indicates significance) to be day, so we add day to our twelve run
f_twelve_run <- add1(glm(as.factor(y)~duration+poutcome+month+contact+housing+job+campaign+loan+marital+education+day, binomial, data = lr.bank.data),
     scope = ~.+age+default+balance+pdays+previous,
     test = "LRT")
#Stop. All p values significant at alpha = 0.05. Drawback = cutoff level is trivial choice. 
#Final mode: y ~ job + marital + education + housing + loan + contact + day + month + duration + campaign + poutcome
#Dropped age, default, pdays, previous, balance.
```

Backward and forward stepwise using LRT yield same results. Dropped age, default, balance, pdays, previous

## Forward AIC - AIC give more complicated model, better predictive performance
```{r, message=FALSE, warning = FALSE}
f_AIC <- step(glm(y~1, binomial, lr.bank.data),
     scope = ~age+job+marital+education+default+balance+housing+loan+contact+day+month+duration+campaign+pdays+previous+poutcome,
     direction = "forward")
#previous, pdays, default, age
# y ~ duration + poutcome + month + contact + housing + job + campaign + loan + marital + education + day + balance
```

## Forward BIC - simple - good for inference
```{r, message=FALSE, warning = FALSE}
f_BIC <- step(glm(y~1, binomial, lr.bank.data),
     scope = ~age+job+marital+education+default+balance+housing+loan+contact+day+month+duration+campaign+pdays+previous+poutcome,
     direction = "forward",
     k = log(dim(lr.bank.data)[1]))
#education, balance, previous, default, age, pdays, job
#y ~ duration + poutcome + month + contact + housing + campaign + marital + loan + day
```

## Backward AIC - AIC give more complicated model, better predictive performance
```{r, message=FALSE, warning = FALSE}
b_AIC <- step(glm(y~., binomial, lr.bank.data),
     direction = "backward")
#pdays, default, age, previous
# y ~ job + marital + education + balance + housing + loan + contact + day + month + duration + campaign + poutcome
```

## Backward BIC - simple - good for inference
```{r, message=FALSE, warning = FALSE}
b_BIC <- step(glm(y~., binomial, lr.bank.data),
     direction = "backward",
     k = log(dim(lr.bank.data)[1]))
#education, balance, previous, default, age, pdays, job
#y ~ marital + housing + loan + contact + day + month + duration + campaign + poutcome
```

## bothward AIC
```{r, message=FALSE, warning = FALSE}
both_AIC <- step(glm(y~1, binomial, lr.bank.data),
     scope = ~age+job+marital+education+default+balance+housing+loan+contact+day+month+duration+campaign+pdays+previous+poutcome,
     direction = "both")
#pdays, default, age, previous
#y ~ duration + poutcome + month + contact + housing + job + campaign + loan + marital + day + education + balance
```

## bothward BIC
```{r, message=FALSE, warning = FALSE}
both_BIC <- step(glm(y~1, binomial, lr.bank.data),
     scope = ~age+job+marital+education+default+balance+housing+loan+contact+day+month+duration+campaign+pdays+previous+poutcome,
     direction = "both",
     k = log(dim(lr.bank.data)[1]))

#y ~ duration + poutcome + month + contact + housing + campaign + marital + loan + day
#education, balance, previous, default, age, pdays, job
```

For logistic regression, we use both forward and backward stepwise selection using AIC as our selection criteria. Although using the Liklihood Ratio Test(LRT) also works, we believe the using a proper parameter such as AIC or BIC would be more robust than using a LRT. Furthermore, we choose to use AIC over BIC. The reason is that we don't mind having a more comlicated model to give us better predictive performance, rather than having a small model as selected by the BIC (which is simple and better for inference as compared to a model selected by BIC.)

Ultimately, we see that all all 3 stepwise selections (backward, forward, both) using AIC yield the exact same model. We have dropped pdays, default, age, and previous. Our final logistic regression model would be

*y ~ duration + poutcome + month + contact + housing + job + campaign + loan + marital + day + education + balance*

## Confusion matrix
```{r}
#Build logistic model with selected significant variables using AIC stepwise variable selection
logistic.model <- glm(y ~ duration + poutcome + month + contact + housing + job + campaign + loan + marital + day + education + balance, binomial(link = "logit"),lr.bank.data)
pred = predict(logistic.model, newdata = bank.test, type = "response")

conf = table(predict(logistic.model, newdata = bank.test, type = "response")$class, bank.test$y)
(conf[1,1]+conf[2,2])/(sum(conf))

confusionMatrix(data = as.numeric(pred>0.5), reference = as.factor(bank.test$y))
```


```{r}
train$y = factor(train$y)
rf <- randomForest(y ~ ., data = train)

z <- lda(AHD ~ ., data = Heart[trainHeart,],prior=c(1,1)/2)
predict(z, Heart[-trainHeart, ])$class
# confusion matrix
z.conf=table(predict(z, Heart[-trainHeart, ])$class,Heart$AHD[-trainHeart])
z.conf
#Classification accuracy
(z.conf[1,1]+z.conf[2,2])/(sum(z.conf))
```

## Animay

```{r, message=FALSE, warning = FALSE}
#Build logistic model with selected significant variables using AIC stepwise variable selection
logistic.model <- glm(y ~ duration + poutcome + month + contact + housing + job + campaign + loan + marital + day + education + balance, binomial(link = "logit"),lr.bank.data)

#prediction
logistic.pred = predict(logistic.model, newdata = bank.test, type = 'response')

#Tuning parameters
pred.tune.log = prediction(logistic.pred, bank.test$y)
perf.tune.log = performance(pred.tune.log, measure = "tpr", x.measure = "fpr")
accuracy.log = performance(pred.tune.log, measure = "acc", x.measure = "cutoff")
plot(accuracy.log)
plot(perf.tune.log) ##selecting cut off calue of 0.3

pred.cutoff = cut(logistic.pred, c(-Inf, 0.2, Inf), labels = c("No","Yes"))
 
##Confusion
glmconf = table(pred.cutoff, bank.test$y)
summary(pred.cutoff)
acc.logistic = (glmconf[1,1]+glmconf[2,2])/(sum(glmconf))
```
-Accuracy = TP+TN/(TP+TN+FP+FN)
Thus accuracy = `r acc.logistic`

### Correlation Analysis
```{r, message=FALSE, warning = FALSE}
bank.num = data.frame(as.numeric(as.factor(bank.data$age)),
                      as.numeric(as.factor(bank.data$job)),
                      as.numeric(as.factor(bank.data$marital)),
                      as.numeric(as.factor(bank.data$education)),
                      as.numeric(as.factor(bank.data$default)),
                      as.numeric(as.factor(bank.data$balance)),
                      as.numeric(as.factor(bank.data$housing)),
                      as.numeric(as.factor(bank.data$loan)),
                      as.numeric(as.factor(bank.data$contact)),
                      as.numeric(as.factor(bank.data$day)),
                      as.numeric(as.factor(bank.data$month)),
                      as.numeric(as.factor(bank.data$duration)),
                      as.numeric(as.factor(bank.data$campaign)),
                      as.numeric(as.factor(bank.data$pdays)),
                      as.numeric(as.factor(bank.data$previous)),
                      as.numeric(as.factor(bank.data$poutcome)),
                      as.numeric(as.factor(bank.data$y)))
                      
colnames(bank.num) = c("Age", "Job", "Marital", "Education", "Default", "Balance", "Housing", " Loan", "Contact", "Day", "Month", "Duration","Campaign", "Pdays", "Previous", "Poutcome","Y")


bank.num %>%
  cor() %>%
  corrplot(method = "number",
           tl.srt = 45,
           bg = "black",
           order = "FPC",
           tl.col = "black",
           number.cex = 0.5)
```
The above correlogram is ordered in a first principle component manner. We can see that for the response variable, i.e. Y, the following can be said for its correlation with respect to the predictor variables:
1.) 

##Random Forest
```{r}
# Install the dependencies and required libs
library(randomForest)
require(caTools)
```

```{r}
# head(bank.data)

# split the dataset into train and test
sample = sample.split(bank.data$y, SplitRatio = .75)
train = subset(bank.data, sample == TRUE)
test  = subset(bank.data, sample == FALSE)

```

```{r}
train$y = factor(train$y)
rf <- randomForest(y ~ ., data = train)

z <- lda(AHD ~ ., data = Heart[trainHeart,],prior=c(1,1)/2)
predict(z, Heart[-trainHeart, ])$class
# confusion matrix
z.conf=table(predict(z, Heart[-trainHeart, ])$class,Heart$AHD[-trainHeart])
z.conf
#Classification accuracy
(z.conf[1,1]+z.conf[2,2])/(sum(z.conf))
```

```{r}
rf
names(rf)
importance(rf)
```


```{r}
library(rpart)
DTmodel <- rpart(y~., data=train, method="class")
DTmodel
```
